---
# Source: clearml-server-cloud-ready/charts/elasticsearch/templates/poddisruptionbudget.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: "clearml-elastic-master-pdb"
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app: "clearml-elastic-master"
---
# Source: clearml-server-cloud-ready/charts/mongodb/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: clearml-server-mongodb
  namespace: default
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-10.3.4
    app.kubernetes.io/instance: clearml-server
    app.kubernetes.io/managed-by: Helm
secrets:
  - name: clearml-server-mongodb
---
# Source: clearml-server-cloud-ready/templates/secret-agent.yaml
apiVersion: v1
kind: Secret
metadata:
  name: agent-group0-conf
data:
  clearml.conf: c2RrIHsKfQ==
---
# Source: clearml-server-cloud-ready/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: clearml-conf
data:
  apiserver_key: NTQ0MkYzNDQzTUpNT1JXWkEzWkg=
  apiserver_secret: QnhhcElSbzlaSU5pOHgyNUNSeHo4V2RtcjJwUWp6dVdWQjRQTkFTWnFDdFR5V2dXVlE=
  http_session: OVR3MjBSYmhKMWJMQmlIRU9XWHZocGxLR1ViVGdMekF0d0ZOMm9MUXZXd1MwdVJwRDU=
  auth_token: MVNDZjBvdjNObTU0NFRkMm9aMGdYU3JzTng1WGhNV2RWbEt6MXRPZ2N4MTU4YkQ1UlY=
  tests_user_key: RU5QMzlFUU00U0xBQ0dENUZYQjc=
  tests_user_secret: bFBjbTBpbWJjQlo4bXdnTzd0cGFkdXRpUzNnbkpEMDV4OWo3YWZ3WFBTMzVJS2JwaVE=
---
# Source: clearml-server-cloud-ready/charts/elasticsearch/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: clearml-elastic-master-config
  labels:
    heritage: "Helm"
    release: "clearml-server"
    chart: "elasticsearch"
    app: "clearml-elastic-master"
data:
  elasticsearch.yml: |
    xpack.security.enabled: false
---
# Source: clearml-server-cloud-ready/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: clearml-server-redis
  namespace: default
  labels:
    app: redis
    chart: redis-10.9.0
    heritage: Helm
    release: clearml-server
data:
  redis.conf: |-
    # User-supplied configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
  master.conf: |-
    dir /data
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
  replica.conf: |-
    dir /data
    slave-read-only yes
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
---
# Source: clearml-server-cloud-ready/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: clearml-server-redis-health
  namespace: default
  labels:
    app: redis
    chart: redis-10.9.0
    heritage: Helm
    release: clearml-server
data:
  ping_readiness_local.sh: |-
    #!/bin/bash
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading the dataset in memory" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    #!/bin/bash
     response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash
    response=$(
      timeout -s 3 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$response" != "PONG" ] && [ "$response" != "LOADING Redis is loading the dataset in memory" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: clearml-server-cloud-ready/charts/mongodb/templates/standalone/pvc.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: clearml-server-mongodb
  namespace: default
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-10.3.4
    app.kubernetes.io/instance: clearml-server
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
spec:
  accessModes:
    - "ReadWriteOnce"
  resources:
    requests:
      storage: "50Gi"
---
# Source: clearml-server-cloud-ready/templates/pvc-agentservices.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: clearml-server-clearml-server-cloud-ready-agentservices-data
  labels:
    helm.sh/chart: clearml-server-cloud-ready-2.0.2_1
    app.kubernetes.io/name: clearml-server-cloud-ready
    app.kubernetes.io/instance: clearml-server
    app.kubernetes.io/version: "1.0.2"
    app.kubernetes.io/managed-by: Helm
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: "50Gi"
  storageClassName: "standard"
---
# Source: clearml-server-cloud-ready/templates/pvc-fileserver.yaml
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: clearml-server-clearml-server-cloud-ready-fileserver-data
  labels:
    helm.sh/chart: clearml-server-cloud-ready-2.0.2_1
    app.kubernetes.io/name: clearml-server-cloud-ready
    app.kubernetes.io/instance: clearml-server
    app.kubernetes.io/version: "1.0.2"
    app.kubernetes.io/managed-by: Helm
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: "50Gi"
  storageClassName: "standard"
---
# Source: clearml-server-cloud-ready/charts/elasticsearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: clearml-elastic-master
  labels:
    heritage: "Helm"
    release: "clearml-server"
    chart: "elasticsearch"
    app: "clearml-elastic-master"
  annotations:
    {}
spec:
  type: ClusterIP
  selector:
    heritage: "Helm"
    release: "clearml-server"
    chart: "elasticsearch"
    app: "clearml-elastic-master"
  ports:
  - name: http
    protocol: TCP
    port: 9200
  - name: transport
    protocol: TCP
    port: 9300
---
# Source: clearml-server-cloud-ready/charts/elasticsearch/templates/service.yaml
kind: Service
apiVersion: v1
metadata:
  name: clearml-elastic-master-headless
  labels:
    heritage: "Helm"
    release: "clearml-server"
    chart: "elasticsearch"
    app: "clearml-elastic-master"
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  clusterIP: None # This is needed for statefulset hostnames like elasticsearch-0 to resolve
  # Create endpoints also if the related pod isn't ready
  publishNotReadyAddresses: true
  selector:
    app: "clearml-elastic-master"
  ports:
  - name: http
    port: 9200
  - name: transport
    port: 9300
---
# Source: clearml-server-cloud-ready/charts/mongodb/templates/standalone/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: clearml-server-mongodb
  namespace: default
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-10.3.4
    app.kubernetes.io/instance: clearml-server
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
spec:
  type: ClusterIP
  ports:
    - name: mongo-service
      port: 27017
      targetPort: mongodb
      nodePort: null
  selector:
    app.kubernetes.io/name: mongodb
    app.kubernetes.io/instance: clearml-server
    app.kubernetes.io/component: mongodb
---
# Source: clearml-server-cloud-ready/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: clearml-server-redis-headless
  namespace: default
  labels:
    app: redis
    chart: redis-10.9.0
    release: clearml-server
    heritage: Helm
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: redis
      port: 6379
      targetPort: redis
  selector:
    app: redis
    release: clearml-server
---
# Source: clearml-server-cloud-ready/charts/redis/templates/redis-master-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: clearml-server-redis-master
  namespace: default
  labels:
    app: redis
    chart: redis-10.9.0
    release: clearml-server
    heritage: Helm
spec:
  type: ClusterIP
  ports:
    - name: redis
      port: 6379
      targetPort: redis
  selector:
    app: redis
    release: clearml-server
    role: master
---
# Source: clearml-server-cloud-ready/templates/service-apiserver.yaml
apiVersion: v1
kind: Service
metadata:
  name: clearml-server-clearml-server-cloud-ready-apiserver
  labels:
    helm.sh/chart: clearml-server-cloud-ready-2.0.2_1
    app.kubernetes.io/name: clearml-server-cloud-ready
    app.kubernetes.io/instance: clearml-server
    app.kubernetes.io/version: "1.0.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: NodePort
  ports:
    - port: 8008
      targetPort: 8008
      nodePort: 30008
      protocol: TCP
  selector:
    app.kubernetes.io/name: clearml-server-cloud-ready
    app.kubernetes.io/instance: clearml-server-apiserver
---
# Source: clearml-server-cloud-ready/templates/service-fileserver.yaml
apiVersion: v1
kind: Service
metadata:
  name: clearml-server-clearml-server-cloud-ready-fileserver
  labels:
    helm.sh/chart: clearml-server-cloud-ready-2.0.2_1
    app.kubernetes.io/name: clearml-server-cloud-ready
    app.kubernetes.io/instance: clearml-server
    app.kubernetes.io/version: "1.0.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: NodePort
  ports:
    - port: 8081
      targetPort: 8081
      nodePort: 30081
      protocol: TCP
  selector:
    app.kubernetes.io/name: clearml-server-cloud-ready
    app.kubernetes.io/instance: clearml-server-fileserver
---
# Source: clearml-server-cloud-ready/templates/service-webserver.yaml
apiVersion: v1
kind: Service
metadata:
  name: clearml-server-clearml-server-cloud-ready-webserver
  labels:
    helm.sh/chart: clearml-server-cloud-ready-2.0.2_1
    app.kubernetes.io/name: clearml-server-cloud-ready
    app.kubernetes.io/instance: clearml-server
    app.kubernetes.io/version: "1.0.2"
    app.kubernetes.io/managed-by: Helm
spec:
  type: NodePort
  ports:
    - port: 80
      targetPort: 80
      nodePort: 30080
      protocol: TCP
  selector:
    app.kubernetes.io/name: clearml-server-cloud-ready
    app.kubernetes.io/instance: clearml-server-webserver
---
# Source: clearml-server-cloud-ready/charts/mongodb/templates/standalone/dep-sts.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: clearml-server-mongodb
  namespace: default
  labels:
    app.kubernetes.io/name: mongodb
    helm.sh/chart: mongodb-10.3.4
    app.kubernetes.io/instance: clearml-server
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: mongodb
spec:
  strategy:
    type: RollingUpdate
  selector:
    matchLabels:
      app.kubernetes.io/name: mongodb
      app.kubernetes.io/instance: clearml-server
      app.kubernetes.io/component: mongodb
  template:
    metadata:
      labels:
        app.kubernetes.io/name: mongodb
        helm.sh/chart: mongodb-10.3.4
        app.kubernetes.io/instance: clearml-server
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: mongodb
    spec:
      
      serviceAccountName: clearml-server-mongodb
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/name: mongodb
                    app.kubernetes.io/instance: clearml-server
                    app.kubernetes.io/component: mongodb
                namespaces:
                  - default
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        sysctls: []
      containers:
        - name: mongodb
          image: docker.io/bitnami/mongodb:3.6.21-debian-9-r71
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsNonRoot: true
            runAsUser: 1001
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: MONGODB_SYSTEM_LOG_VERBOSITY
              value: "0"
            - name: MONGODB_DISABLE_SYSTEM_LOG
              value: "no"
            - name: MONGODB_ENABLE_IPV6
              value: "no"
            - name: MONGODB_ENABLE_DIRECTORY_PER_DB
              value: "no"
          ports:
            - name: mongodb
              containerPort: 27017
          livenessProbe:
            exec:
              command:
                - mongo
                - --eval
                - "db.adminCommand('ping')"
            initialDelaySeconds: 30
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          readinessProbe:
            exec:
              command:
                - mongo
                - --eval
                - "db.adminCommand('ping')"
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 6
          resources:
            limits: {}
            requests: {}
          volumeMounts:
            - name: datadir
              mountPath: /bitnami/mongodb
              subPath: 
      volumes:
        - name: datadir
          persistentVolumeClaim:
            claimName: clearml-server-mongodb
---
# Source: clearml-server-cloud-ready/templates/deployment-agent.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: clearml-server-clearml-server-cloud-ready-agent-group0-agent
  labels:
    helm.sh/chart: clearml-server-cloud-ready-2.0.2_1
    app.kubernetes.io/name: clearml-server-cloud-ready
    app.kubernetes.io/instance: clearml-server
    app.kubernetes.io/version: "1.0.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 0
  selector:
    matchLabels:
      app.kubernetes.io/name: clearml-server-cloud-ready
      app.kubernetes.io/instance: clearml-server-agent
  template:
    metadata:
      labels:
        app.kubernetes.io/name: clearml-server-cloud-ready
        app.kubernetes.io/instance: clearml-server-agent
    spec:
      volumes:
        
        - name: agent-clearml-conf-volume
          secret:
            secretName: agent-group0-conf
            items:
            - key: clearml.conf
              path: clearml.conf
        
      initContainers:
        - name: init-agent-agent-group0
          image: "nvidia/cuda:11.0-base-ubuntu18.04"
          command:
            - /bin/sh
            - -c
            - >
              set -x;
              while [ $(curl -sw '%{http_code}' "http://clearml-server-clearml-server-cloud-ready-apiserver:8008/debug.ping" -o /dev/null) -ne 200 ] ; do
                echo "waiting for apiserver" ;
                sleep 5 ;
              done
      containers:
        - name: clearml-server-cloud-ready-agent-group0
          image: "nvidia/cuda:11.0-base-ubuntu18.04"
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: true
          resources:
            limits:
              nvidia.com/gpu:
                1
          env:
          - name: CLEARML_API_HOST
            value: 'http://clearml-server-clearml-server-cloud-ready-apiserver:8008'
          - name: CLEARML_WEB_HOST
            value: 'http://clearml-server-clearml-server-cloud-ready-webserver:80'
          - name: CLEARML_FILES_HOST
            value: 'http://clearml-server-clearml-server-cloud-ready-fileserver:8081'
          - name: CLEARML_AGENT_GIT_USER
            value: 
          - name: CLEARML_AGENT_GIT_PASS
            value: 
          - name: AWS_ACCESS_KEY_ID
            value: 
          - name: AWS_SECRET_ACCESS_KEY
            value: 
          - name: AWS_DEFAULT_REGION
            value: 
          - name: AZURE_STORAGE_ACCOUNT
            value: 
          - name: AZURE_STORAGE_KEY
            value: 
          - name: CLEARML_API_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: clearml-conf
                key: tests_user_key
          - name: CLEARML_API_SECRET_KEY
            valueFrom:
              secretKeyRef:
                name: clearml-conf
                key: tests_user_secret
          command:
            - /bin/sh
            - -c
            - "apt-get update ;
              apt-get install -y curl python3-pip git;
              python3 -m pip install -U pip ;
              python3 -m pip install clearml-agent ;
              CLEARML_AGENT_K8S_HOST_MOUNT=/root/.clearml:/root/.clearml clearml-agent daemon --queue default"
---
# Source: clearml-server-cloud-ready/templates/deployment-agentservices.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: clearml-server-clearml-server-cloud-ready-agentservices
  labels:
    helm.sh/chart: clearml-server-cloud-ready-2.0.2_1
    app.kubernetes.io/name: clearml-server-cloud-ready
    app.kubernetes.io/instance: clearml-server
    app.kubernetes.io/version: "1.0.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: clearml-server-cloud-ready
      app.kubernetes.io/instance: clearml-server-agentservices
  template:
    metadata:
      labels:
        app.kubernetes.io/name: clearml-server-cloud-ready
        app.kubernetes.io/instance: clearml-server-agentservices
    spec:
      volumes:
        - name: agentservices-data
          persistentVolumeClaim:
            claimName: clearml-server-clearml-server-cloud-ready-agentservices-data
      initContainers:
          - name: init-agentservices
            image: "allegroai/clearml-agent-services:latest"
            command:
              - /bin/sh
              - -c
              - >
                set -x;
                while [ $(curl -sw '%{http_code}' "http://clearml-server-clearml-server-cloud-ready-apiserver:8008/debug.ping" -o /dev/null) -ne 200 ] ; do
                  echo "waiting for apiserver" ;
                  sleep 5 ;
                done
      containers:
        - name: clearml-server-cloud-ready
          image: "allegroai/clearml-agent-services:latest"
          imagePullPolicy: IfNotPresent
          env:
          - name: CLEARML_HOST_IP
            value: 
          - name: CLEARML_API_HOST
            value: "http://clearml-server-clearml-server-cloud-ready-apiserver:8008"
          - name: CLEARML_WEB_HOST
            value: 
          - name: CLEARML_FILES_HOST
            value: 
          - name: CLEARML_AGENT_GIT_USER
            value: 
          - name: CLEARML_AGENT_GIT_PASS
            value: 
          - name: CLEARML_AGENT_UPDATE_VERSION
            value: 
          - name: CLEARML_AGENT_DEFAULT_BASE_DOCKER
            value: 
          - name: AWS_ACCESS_KEY_ID
            value: 
          - name: AWS_SECRET_ACCESS_KEY
            value: 
          - name: AWS_DEFAULT_REGION
            value: 
          - name: AZURE_STORAGE_ACCOUNT
            value: 
          - name: AZURE_STORAGE_KEY
            value: 
          - name: GOOGLE_APPLICATION_CREDENTIALS
            value: 
          - name: CLEARML_WORKER_ID
            value: clearml-services
          - name: CLEARML_API_ACCESS_KEY
            valueFrom:
              secretKeyRef:
                name: clearml-conf
                key: tests_user_key
          - name: CLEARML_API_SECRET_KEY
            valueFrom:
              secretKeyRef:
                name: clearml-conf
                key: tests_user_secret
          args:
          - agentservices
          volumeMounts:
            - name: agentservices-data
              mountPath: /root/.clearml
          resources:
            {}
---
# Source: clearml-server-cloud-ready/templates/deployment-apiserver.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: clearml-server-clearml-server-cloud-ready-apiserver
  labels:
    helm.sh/chart: clearml-server-cloud-ready-2.0.2_1
    app.kubernetes.io/name: clearml-server-cloud-ready
    app.kubernetes.io/instance: clearml-server
    app.kubernetes.io/version: "1.0.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: clearml-server-cloud-ready
      app.kubernetes.io/instance: clearml-server-apiserver
  template:
    metadata:
      labels:
        app.kubernetes.io/name: clearml-server-cloud-ready
        app.kubernetes.io/instance: clearml-server-apiserver
    spec:
      containers:
        - name: clearml-server-cloud-ready
          image: "allegroai/clearml:1.0.2"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8008
              protocol: TCP
          env:
          - name: CLEARML_ELASTIC_SERVICE_HOST
            value: "clearml-server-elastic-master"
          - name: CLEARML_ELASTIC_SERVICE_PORT
            value: "9200"
          - name: CLEARML_MONGODB_SERVICE_HOST
            value: "clearml-server-mongodb"
          - name: CLEARML_MONGODB_SERVICE_PORT
            value: "27017"
          - name: CLEARML_REDIS_SERVICE_HOST
            value: "clearml-server-redis-master"
          - name: CLEARML_REDIS_SERVICE_PORT
            value: "6379"
          - name: CLEARML__APISERVER__PRE_POPULATE__ENABLED
            value: "true"
          - name: CLEARML__APISERVER__PRE_POPULATE__ZIP_FILES
            value: "/opt/clearml/db-pre-populate"
          - name: CLEARML_SERVER_DEPLOYMENT_TYPE
            value: "helm-cloud"
          - name: CLEARML_CONFIG_DIR
            value: /opt/clearml/config
          - name: CLEARML__APISERVER__DEFAULT_COMPANY
            value: d1bd92a3b039400cbafc60a7a5b1e52b
          - name: CLEARML__SECURE__HTTP__SESSION_SECRET__APISERVER
            valueFrom:
              secretKeyRef:
                name: clearml-conf
                key: http_session
          - name: CLEARML__SECURE__AUTH__TOKEN_SECRET
            valueFrom:
              secretKeyRef:
                name: clearml-conf
                key: auth_token
          - name: CLEARML__SECURE__CREDENTIALS__APISERVER__USER_KEY
            valueFrom:
              secretKeyRef:
                name: clearml-conf
                key: apiserver_key
          - name: CLEARML__SECURE__CREDENTIALS__APISERVER__USER_SECRET
            valueFrom:
              secretKeyRef:
                name: clearml-conf
                key: apiserver_secret
          - name: CLEARML__SECURE__CREDENTIALS__TESTS__USER_KEY
            valueFrom:
              secretKeyRef:
                name: clearml-conf
                key: tests_user_key
          - name: CLEARML__SECURE__CREDENTIALS__TESTS__USER_SECRET
            valueFrom:
              secretKeyRef:
                name: clearml-conf
                key: tests_user_secret
          args:
            - apiserver
          livenessProbe:
            initialDelaySeconds: 60
            httpGet:
              path: /debug.ping
              port: 8008
          readinessProbe:
            initialDelaySeconds: 60
            failureThreshold: 8
            httpGet:
              path: /debug.ping
              port: 8008
          resources:
            {}
---
# Source: clearml-server-cloud-ready/templates/deployment-fileserver.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: clearml-server-clearml-server-cloud-ready-fileserver
  labels:
    helm.sh/chart: clearml-server-cloud-ready-2.0.2_1
    app.kubernetes.io/name: clearml-server-cloud-ready
    app.kubernetes.io/instance: clearml-server
    app.kubernetes.io/version: "1.0.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: clearml-server-cloud-ready
      app.kubernetes.io/instance: clearml-server-fileserver
  template:
    metadata:
      labels:
        app.kubernetes.io/name: clearml-server-cloud-ready
        app.kubernetes.io/instance: clearml-server-fileserver
    spec:
      volumes:
        - name: fileserver-data
          persistentVolumeClaim:
            claimName: clearml-server-clearml-server-cloud-ready-fileserver-data
      containers:
        - name: clearml-server-cloud-ready
          image: "allegroai/clearml:1.0.2"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8081
              protocol: TCP
          env:
          - name: CLEARML_CONFIG_DIR
            value: /opt/clearml/config
          args:
            - fileserver
          livenessProbe:
            exec:
              command:
                - curl
                - -X OPTIONS
                - http://localhost:8081/
          readinessProbe:
            exec:
              command:
                - curl
                - -X OPTIONS
                - http://localhost:8081/
          volumeMounts:
            - name: fileserver-data
              mountPath: /mnt/fileserver
          resources:
            {}
---
# Source: clearml-server-cloud-ready/templates/deployment-webserver.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: clearml-server-clearml-server-cloud-ready-webserver
  labels:
    helm.sh/chart: clearml-server-cloud-ready-2.0.2_1
    app.kubernetes.io/name: clearml-server-cloud-ready
    app.kubernetes.io/instance: clearml-server
    app.kubernetes.io/version: "1.0.2"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: clearml-server-cloud-ready
      app.kubernetes.io/instance: clearml-server-webserver
  template:
    metadata:
      labels:
        app.kubernetes.io/name: clearml-server-cloud-ready
        app.kubernetes.io/instance: clearml-server-webserver
    spec:
      containers:
        - name: clearml-server-cloud-ready
          image: "allegroai/clearml:1.0.2"
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 80
              protocol: TCP
          livenessProbe:
            exec:
              command:
                - curl
                - -X OPTIONS
                - http://0.0.0.0:80/
          readinessProbe:
            exec:
              command:
                - curl
                - -X OPTIONS
                - http://0.0.0.0:80/
          env:
          - name: NGINX_APISERVER_ADDRESS
            value: "http://clearml-server-clearml-server-cloud-ready-apiserver:8008"
          - name: NGINX_FILESERVER_ADDRESS
            value: "http://clearml-server-clearml-server-cloud-ready-fileserver:8081"
          args:
            - webserver
          resources:
            {}
---
# Source: clearml-server-cloud-ready/charts/elasticsearch/templates/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: clearml-elastic-master
  labels:
    heritage: "Helm"
    release: "clearml-server"
    chart: "elasticsearch"
    app: "clearml-elastic-master"
  annotations:
    esMajorVersion: "7"
spec:
  serviceName: clearml-elastic-master-headless
  selector:
    matchLabels:
      app: "clearml-elastic-master"
  replicas: 1
  podManagementPolicy: Parallel
  updateStrategy:
    type: RollingUpdate
  volumeClaimTemplates:
  - metadata:
      name: clearml-elastic-master
    spec:
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 50Gi
  template:
    metadata:
      name: "clearml-elastic-master"
      labels:
        heritage: "Helm"
        release: "clearml-server"
        chart: "elasticsearch"
        app: "clearml-elastic-master"
      annotations:
        
        configchecksum: 74bf3a32b86b711225b81f59050eb46d9c7e332399326f6fd4ee8627b4febfa
    spec:
      securityContext:
        fsGroup: 1000
        runAsUser: 1000
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - "clearml-elastic-master"
            topologyKey: kubernetes.io/hostname
      terminationGracePeriodSeconds: 120
      volumes:
        - name: esconfig
          configMap:
            name: clearml-elastic-master-config
      initContainers:
      - name: configure-sysctl
        securityContext:
          runAsUser: 0
          privileged: true
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.6.2"
        imagePullPolicy: "IfNotPresent"
        command: ["sysctl", "-w", "vm.max_map_count=262144"]
        resources:
          {}

      containers:
      - name: "elasticsearch"
        securityContext:
          capabilities:
            drop:
            - ALL
          runAsNonRoot: true
          runAsUser: 1000
        image: "docker.elastic.co/elasticsearch/elasticsearch:7.6.2"
        imagePullPolicy: "IfNotPresent"
        readinessProbe:
          exec:
            command:
              - sh
              - -c
              - |
                #!/usr/bin/env bash -e
                # If the node is starting up wait for the cluster to be ready (request params: 'wait_for_status=yellow&timeout=1s' )
                # Once it has started only check that the node itself is responding
                START_FILE=/tmp/.es_start_file

                http () {
                    local path="${1}"
                    if [ -n "${ELASTIC_USERNAME}" ] && [ -n "${ELASTIC_PASSWORD}" ]; then
                      BASIC_AUTH="-u ${ELASTIC_USERNAME}:${ELASTIC_PASSWORD}"
                    else
                      BASIC_AUTH=''
                    fi
                    curl -XGET -s -k --fail ${BASIC_AUTH} http://127.0.0.1:9200${path}
                }

                if [ -f "${START_FILE}" ]; then
                    echo 'Elasticsearch is already running, lets check the node is healthy and there are master nodes available'
                    http "/_cluster/health?timeout=0s"
                else
                    echo 'Waiting for elasticsearch cluster to become ready (request params: "wait_for_status=yellow&timeout=1s" )'
                    if http "/_cluster/health?wait_for_status=yellow&timeout=1s" ; then
                        touch ${START_FILE}
                        exit 0
                    else
                        echo 'Cluster is not yet ready (request params: "wait_for_status=yellow&timeout=1s" )'
                        exit 1
                    fi
                fi
          failureThreshold: 3
          initialDelaySeconds: 10
          periodSeconds: 10
          successThreshold: 3
          timeoutSeconds: 5
        ports:
        - name: http
          containerPort: 9200
        - name: transport
          containerPort: 9300
        resources:
          limits:
            cpu: 1000m
            memory: 4Gi
          requests:
            cpu: 1000m
            memory: 4Gi
        env:
          - name: node.name
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: cluster.initial_master_nodes
            value: "clearml-elastic-master-0,"
          - name: discovery.seed_hosts
            value: "clearml-elastic-master-headless"
          - name: cluster.name
            value: "clearml-elastic"
          - name: network.host
            value: "0.0.0.0"
          - name: ES_JAVA_OPTS
            value: "-Xmx2g -Xms2g"
          - name: node.data
            value: "true"
          - name: node.ingest
            value: "true"
          - name: node.master
            value: "true"
          - name: bootstrap.memory_lock
            value: "true"
          - name: cluster.routing.allocation.node_initial_primaries_recoveries
            value: "500"
          - name: cluster.routing.allocation.disk.watermark.low
            value: 500mb
          - name: cluster.routing.allocation.disk.watermark.high
            value: 500mb
          - name: cluster.routing.allocation.disk.watermark.flood_stage
            value: 500mb
          - name: http.compression_level
            value: "7"
          - name: reindex.remote.whitelist
            value: '*.*'
          - name: xpack.monitoring.enabled
            value: "false"
          - name: xpack.security.enabled
            value: "false"
        volumeMounts:
          - name: "clearml-elastic-master"
            mountPath: /usr/share/elasticsearch/data

          - name: esconfig
            mountPath: /usr/share/elasticsearch/config/elasticsearch.yml
            subPath: elasticsearch.yml
---
# Source: clearml-server-cloud-ready/charts/redis/templates/redis-master-statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: clearml-server-redis-master
  namespace: default
  labels:
    app: redis
    chart: redis-10.9.0
    release: clearml-server
    heritage: Helm
spec:
  selector:
    matchLabels:
      app: redis
      release: clearml-server
      role: master
  serviceName: clearml-server-redis-headless
  template:
    metadata:
      labels:
        app: redis
        chart: redis-10.9.0
        release: clearml-server
        role: master
      annotations:
        checksum/health: 9ab4fd2379a202fbb074d797cf435c53ae20baed2fe246100cafb18848a88b76
        checksum/configmap: aab37cbb27ed5e93ca78e71d7cd22b393835075c7273568cf88ac17ef10d277d
        checksum/secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855
    spec:
      
      securityContext:
        fsGroup: 1001
      serviceAccountName: default
      containers:
        - name: redis
          image: docker.io/bitnami/redis:5.0.10-debian-10-r88
          imagePullPolicy: "IfNotPresent"
          securityContext:
            runAsUser: 1001
          command:
            - /bin/bash
            - -c
            - |
              if [[ -n $REDIS_PASSWORD_FILE ]]; then
                password_aux=`cat ${REDIS_PASSWORD_FILE}`
                export REDIS_PASSWORD=$password_aux
              fi
              if [[ ! -f /opt/bitnami/redis/etc/master.conf ]];then
                cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
              fi
              if [[ ! -f /opt/bitnami/redis/etc/redis.conf ]];then
                cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
              fi
              ARGS=("--port" "${REDIS_PORT}")
              ARGS+=("--protected-mode" "no")
              ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
              ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
              /run.sh ${ARGS[@]}
          env:
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "yes"
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          livenessProbe:
            initialDelaySeconds: 5
            periodSeconds: 5
            # One second longer than command timeout should prevent generation of zombie processes.
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          readinessProbe:
            initialDelaySeconds: 5
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
          resources:
            null
          volumeMounts:
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
              subPath: 
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: redis-tmp-conf
              mountPath: /opt/bitnami/redis/etc/
      volumes:
        - name: health
          configMap:
            name: clearml-server-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: clearml-server-redis
        - name: redis-tmp-conf
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: redis-data
        labels:
          app: redis
          release: clearml-server
          heritage: Helm
          component: master
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "5Gi"
        
        selector:
  updateStrategy:
    type: RollingUpdate
---
# Source: clearml-server-cloud-ready/charts/elasticsearch/templates/test/test-elasticsearch-health.yaml
apiVersion: v1
kind: Pod
metadata:
  name: "clearml-server-fgtsq-test"
  annotations:
    "helm.sh/hook": test-success
spec:
  containers:
  - name: "clearml-server-zcsfi-test"
    image: "docker.elastic.co/elasticsearch/elasticsearch:7.6.2"
    command:
      - "sh"
      - "-c"
      - |
        #!/usr/bin/env bash -e
        curl -XGET --fail 'clearml-elastic-master:9200/_cluster/health?wait_for_status=yellow&timeout=1s'
  restartPolicy: Never
